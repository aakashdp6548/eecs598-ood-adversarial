03/18/2023 00:12:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/processors/glue.py:475: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format("processor"), FutureWarning)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/18/2023 00:12:26 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='glue_data/QNLI/processed', model_type='distilbert', model_name_or_path='distilbert-base-uncased', task_name='qnli', output_dir='checkpoints/', config_name='', tokenizer_name='', cache_dir='', max_seq_length=512, do_train=True, do_eval=True, evaluate_during_training=True, do_lower_case=True, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=1e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1, warmup_steps=1986, logging_steps=100, save_steps=100, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', adv_lr=0.05, adv_steps=3, adv_init_mag=0.1, norm_type='l2', adv_max_norm=0.0, gpu='0', expname='default', comet=False, comet_key='', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0, n_gpu=1, device=device(type='cuda'), output_mode='classification')
03/18/2023 00:12:26 - INFO - __main__ -   Loading features from cached file glue_data/QNLI/processed/cached_train_distilbert-base-uncased_512_qnli
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
03/18/2023 00:12:54 - INFO - __main__ -   ***** Running training *****
03/18/2023 00:12:54 - INFO - __main__ -     Num examples = 104742
03/18/2023 00:12:54 - INFO - __main__ -     Num Epochs = 1
03/18/2023 00:12:54 - INFO - __main__ -     Instantaneous batch size per GPU = 8
03/18/2023 00:12:54 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
03/18/2023 00:12:54 - INFO - __main__ -     Gradient Accumulation steps = 4
03/18/2023 00:12:54 - INFO - __main__ -     Total optimization steps = 1
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/13093 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/13093 [00:00<1:37:08,  2.25it/s][A
Iteration:   0%|          | 3/13093 [00:00<34:52,  6.25it/s]  [A
Iteration:   0%|          | 5/13093 [00:00<30:12,  7.22it/s][A
Iteration:   0%|          | 7/13093 [00:00<22:46,  9.58it/s][AIteration:   0%|          | 7/13093 [00:01<31:32,  6.91it/s]
Epoch:   0%|          | 0/1 [00:01<?, ?it/s]
03/18/2023 00:12:55 - INFO - __main__ -    global_step = 2, average loss = 0.7019633091986179
03/18/2023 00:12:55 - INFO - __main__ -   Saving model checkpoint to checkpoints/
03/18/2023 00:12:57 - INFO - __main__ -   Evaluate the following checkpoints: ['checkpoints/']
03/18/2023 00:12:58 - INFO - __main__ -   Creating features from dataset file at glue_data/QNLI/processed
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format("function"), FutureWarning)
03/18/2023 00:13:03 - INFO - __main__ -   Saving features into cached file glue_data/QNLI/processed/cached_dev_distilbert-base-uncased_512_qnli
03/18/2023 00:13:06 - INFO - __main__ -   ***** Running evaluation  *****
03/18/2023 00:13:06 - INFO - __main__ -     Num examples = 5462
03/18/2023 00:13:06 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/683 [00:00<?, ?it/s]Evaluating:   2%|▏         | 12/683 [00:00<00:05, 113.83it/s]Evaluating:   4%|▍         | 26/683 [00:00<00:05, 124.23it/s]Evaluating:   6%|▌         | 40/683 [00:00<00:05, 128.38it/s]Evaluating:   8%|▊         | 53/683 [00:00<00:04, 126.86it/s]Evaluating:  10%|▉         | 66/683 [00:00<00:04, 123.84it/s]Evaluating:  12%|█▏        | 80/683 [00:00<00:04, 127.84it/s]Evaluating:  14%|█▍        | 94/683 [00:00<00:04, 128.35it/s]Evaluating:  16%|█▌        | 107/683 [00:00<00:04, 127.49it/s]Evaluating:  18%|█▊        | 121/683 [00:00<00:04, 128.27it/s]Evaluating:  20%|█▉        | 135/683 [00:01<00:04, 130.25it/s]Evaluating:  22%|██▏       | 149/683 [00:01<00:04, 128.32it/s]Evaluating:  24%|██▍       | 163/683 [00:01<00:03, 131.23it/s]Evaluating:  26%|██▌       | 177/683 [00:01<00:03, 129.13it/s]Evaluating:  28%|██▊       | 192/683 [00:01<00:03, 133.47it/s]Evaluating:  30%|███       | 206/683 [00:01<00:03, 133.60it/s]Evaluating:  32%|███▏      | 220/683 [00:01<00:03, 134.50it/s]Evaluating:  34%|███▍      | 234/683 [00:01<00:03, 132.04it/s]Evaluating:  36%|███▋      | 248/683 [00:01<00:03, 131.01it/s]Evaluating:  38%|███▊      | 262/683 [00:02<00:03, 132.12it/s]Evaluating:  40%|████      | 276/683 [00:02<00:03, 131.65it/s]Evaluating:  42%|████▏     | 290/683 [00:02<00:02, 131.65it/s]Evaluating:  45%|████▍     | 304/683 [00:02<00:02, 131.59it/s]Evaluating:  47%|████▋     | 318/683 [00:02<00:02, 126.65it/s]Evaluating:  48%|████▊     | 331/683 [00:02<00:02, 126.43it/s]Evaluating:  51%|█████     | 346/683 [00:02<00:02, 132.57it/s]Evaluating:  53%|█████▎    | 360/683 [00:02<00:02, 132.42it/s]Evaluating:  55%|█████▍    | 374/683 [00:02<00:02, 131.20it/s]Evaluating:  57%|█████▋    | 388/683 [00:02<00:02, 132.00it/s]Evaluating:  59%|█████▉    | 402/683 [00:03<00:02, 129.63it/s]Evaluating:  61%|██████    | 415/683 [00:03<00:02, 129.44it/s]Evaluating:  63%|██████▎   | 428/683 [00:03<00:01, 128.98it/s]Evaluating:  65%|██████▍   | 442/683 [00:03<00:01, 129.34it/s]Evaluating:  67%|██████▋   | 457/683 [00:03<00:01, 133.16it/s]Evaluating:  69%|██████▉   | 471/683 [00:03<00:01, 132.31it/s]Evaluating:  71%|███████   | 485/683 [00:03<00:01, 133.76it/s]Evaluating:  73%|███████▎  | 500/683 [00:03<00:01, 135.28it/s]Evaluating:  75%|███████▌  | 514/683 [00:03<00:01, 135.03it/s]Evaluating:  77%|███████▋  | 528/683 [00:04<00:01, 135.45it/s]Evaluating:  79%|███████▉  | 542/683 [00:04<00:01, 135.73it/s]Evaluating:  81%|████████▏ | 556/683 [00:04<00:00, 136.32it/s]Evaluating:  83%|████████▎ | 570/683 [00:04<00:00, 134.83it/s]Evaluating:  86%|████████▌ | 584/683 [00:04<00:00, 134.87it/s]Evaluating:  88%|████████▊ | 598/683 [00:04<00:00, 134.71it/s]Evaluating:  90%|████████▉ | 612/683 [00:04<00:00, 131.93it/s]Evaluating:  92%|█████████▏| 627/683 [00:04<00:00, 133.59it/s]Evaluating:  94%|█████████▍| 642/683 [00:04<00:00, 135.95it/s]Evaluating:  96%|█████████▌| 656/683 [00:04<00:00, 134.34it/s]Evaluating:  98%|█████████▊| 670/683 [00:05<00:00, 134.05it/s]Evaluating: 100%|██████████| 683/683 [00:05<00:00, 131.79it/s]
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
03/18/2023 00:13:13 - INFO - __main__ -   ***** Eval results  *****
03/18/2023 00:13:13 - INFO - __main__ -     acc = 0.49340900768949103
