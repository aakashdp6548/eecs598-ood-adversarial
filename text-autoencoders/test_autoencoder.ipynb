{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8353325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6a2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b9bc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefa9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main\n",
    "from argparse import Namespace\n",
    "import test\n",
    "from vocab import Vocab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc189e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train': 'data/mnli/train.txt',\n",
    "    'valid': 'data/mnli/dev.txt',\n",
    "    'model_type': 'aae',\n",
    "    'lambda_adv': 10,\n",
    "    'lambda_p': 0,\n",
    "    'lambda_kl': 0,\n",
    "    'noise': [0.3, 0, 0, 0],\n",
    "    'save_dir': 'checkpoints/aae',\n",
    "    'epochs': 5,\n",
    "    'load_model': '',\n",
    "    'vocab_size': 50000,\n",
    "    'dim_z': 128,\n",
    "    'dim_emb': 512,\n",
    "    'dim_h': 1024,\n",
    "    'nlayers': 1,\n",
    "    'dim_d': 512,\n",
    "    'dropout': 0,\n",
    "    'lr': 0.0005,\n",
    "    'batch_size': 256,\n",
    "    'seed': 598,\n",
    "    'log_interval': 100,\n",
    "    'no_cuda': False,\n",
    "}\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e86dd5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train='data/mnli/train.txt', valid='data/mnli/dev.txt', model_type='aae', lambda_adv=10, lambda_p=0, lambda_kl=0, noise=[0.3, 0, 0, 0], save_dir='checkpoints/aae', epochs=5, load_model='', vocab_size=50000, dim_z=128, dim_emb=512, dim_h=1024, nlayers=1, dim_d=512, dropout=0, lr=0.0005, batch_size=256, seed=598, log_interval=100, no_cuda=False)\n",
      "# train sents 392702, tokens 3916049\n",
      "# valid sents 9815, tokens 97329\n",
      "# vocab size 50005\n",
      "# model parameters: 96413782\n",
      "--------------------------------------------------------------------------------\n",
      "| epoch   1 |   100/ 1572 batches | rec 87.42, adv 0.97, |lvar| 167.16, loss_d 1.24, loss 97.09,\n",
      "| epoch   1 |   200/ 1572 batches | rec 85.16, adv 1.14, |lvar| 294.03, loss_d 1.60, loss 96.55,\n",
      "| epoch   1 |   300/ 1572 batches | rec 79.42, adv 0.74, |lvar| 405.14, loss_d 1.44, loss 86.77,\n",
      "| epoch   1 |   400/ 1572 batches | rec 85.23, adv 0.71, |lvar| 314.87, loss_d 1.33, loss 92.29,\n",
      "| epoch   1 |   500/ 1572 batches | rec 73.99, adv 0.61, |lvar| 245.35, loss_d 1.39, loss 80.06,\n",
      "| epoch   1 |   600/ 1572 batches | rec 78.27, adv 0.67, |lvar| 223.30, loss_d 1.35, loss 84.94,\n",
      "| epoch   1 |   700/ 1572 batches | rec 86.11, adv 0.73, |lvar| 220.77, loss_d 1.40, loss 93.36,\n",
      "| epoch   1 |   800/ 1572 batches | rec 73.30, adv 0.70, |lvar| 226.10, loss_d 1.44, loss 80.29,\n",
      "| epoch   1 |   900/ 1572 batches | rec 70.29, adv 0.68, |lvar| 249.68, loss_d 1.42, loss 77.09,\n",
      "| epoch   1 |  1000/ 1572 batches | rec 77.97, adv 0.69, |lvar| 213.81, loss_d 1.41, loss 84.83,\n",
      "| epoch   1 |  1100/ 1572 batches | rec 85.10, adv 0.93, |lvar| 315.96, loss_d 1.34, loss 94.41,\n",
      "| epoch   1 |  1200/ 1572 batches | rec 80.28, adv 0.76, |lvar| 305.25, loss_d 1.36, loss 87.89,\n",
      "| epoch   1 |  1300/ 1572 batches | rec 70.92, adv 0.69, |lvar| 274.91, loss_d 1.44, loss 77.83,\n",
      "| epoch   1 |  1400/ 1572 batches | rec 65.01, adv 0.66, |lvar| 249.49, loss_d 1.41, loss 71.59,\n",
      "| epoch   1 |  1500/ 1572 batches | rec 68.82, adv 0.62, |lvar| 240.99, loss_d 1.44, loss 75.04,\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time   191s | valid rec 66.20, adv 0.56, |lvar| 204.01, loss_d 1.61, loss 71.85, | saving model\n",
      "--------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 1572 batches | rec 65.54, adv 0.63, |lvar| 277.17, loss_d 1.49, loss 71.80,\n",
      "| epoch   2 |   200/ 1572 batches | rec 65.67, adv 0.64, |lvar| 284.02, loss_d 1.47, loss 72.03,\n",
      "| epoch   2 |   300/ 1572 batches | rec 66.31, adv 0.67, |lvar| 291.70, loss_d 1.43, loss 73.03,\n",
      "| epoch   2 |   400/ 1572 batches | rec 62.99, adv 0.68, |lvar| 273.21, loss_d 1.43, loss 69.74,\n",
      "| epoch   2 |   500/ 1572 batches | rec 73.46, adv 0.65, |lvar| 250.82, loss_d 1.42, loss 79.98,\n",
      "| epoch   2 |   600/ 1572 batches | rec 65.88, adv 0.67, |lvar| 274.15, loss_d 1.41, loss 72.62,\n",
      "| epoch   2 |   700/ 1572 batches | rec 60.66, adv 0.70, |lvar| 278.54, loss_d 1.44, loss 67.61,\n",
      "| epoch   2 |   800/ 1572 batches | rec 69.41, adv 0.67, |lvar| 262.98, loss_d 1.41, loss 76.09,\n",
      "| epoch   2 |   900/ 1572 batches | rec 65.24, adv 0.70, |lvar| 269.26, loss_d 1.41, loss 72.20,\n",
      "| epoch   2 |  1000/ 1572 batches | rec 64.47, adv 0.69, |lvar| 295.00, loss_d 1.40, loss 71.32,\n",
      "| epoch   2 |  1100/ 1572 batches | rec 61.55, adv 0.68, |lvar| 309.56, loss_d 1.44, loss 68.40,\n",
      "| epoch   2 |  1200/ 1572 batches | rec 63.98, adv 0.69, |lvar| 298.39, loss_d 1.42, loss 70.84,\n",
      "| epoch   2 |  1300/ 1572 batches | rec 62.05, adv 0.68, |lvar| 303.31, loss_d 1.42, loss 68.82,\n",
      "| epoch   2 |  1400/ 1572 batches | rec 64.19, adv 0.69, |lvar| 304.66, loss_d 1.42, loss 71.10,\n",
      "| epoch   2 |  1500/ 1572 batches | rec 67.18, adv 0.68, |lvar| 299.67, loss_d 1.40, loss 74.01,\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time   192s | valid rec 57.54, adv 0.67, |lvar| 273.05, loss_d 1.35, loss 64.23, | saving model\n",
      "--------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 1572 batches | rec 67.03, adv 0.71, |lvar| 298.78, loss_d 1.40, loss 74.09,\n",
      "| epoch   3 |   200/ 1572 batches | rec 54.23, adv 0.69, |lvar| 353.23, loss_d 1.44, loss 61.12,\n",
      "| epoch   3 |   300/ 1572 batches | rec 59.32, adv 0.69, |lvar| 393.41, loss_d 1.42, loss 66.23,\n",
      "| epoch   3 |   400/ 1572 batches | rec 57.57, adv 0.68, |lvar| 423.36, loss_d 1.46, loss 64.33,\n",
      "| epoch   3 |   500/ 1572 batches | rec 60.38, adv 0.67, |lvar| 416.85, loss_d 1.42, loss 67.13,\n",
      "| epoch   3 |   600/ 1572 batches | rec 57.30, adv 0.69, |lvar| 394.76, loss_d 1.42, loss 64.21,\n",
      "| epoch   3 |   700/ 1572 batches | rec 65.16, adv 0.68, |lvar| 406.78, loss_d 1.41, loss 71.94,\n",
      "| epoch   3 |   800/ 1572 batches | rec 59.56, adv 0.68, |lvar| 405.57, loss_d 1.41, loss 66.36,\n",
      "| epoch   3 |   900/ 1572 batches | rec 61.79, adv 0.70, |lvar| 398.91, loss_d 1.41, loss 68.79,\n",
      "| epoch   3 |  1000/ 1572 batches | rec 57.60, adv 0.72, |lvar| 398.28, loss_d 1.41, loss 64.79,\n",
      "| epoch   3 |  1100/ 1572 batches | rec 63.82, adv 0.69, |lvar| 421.51, loss_d 1.38, loss 70.77,\n",
      "| epoch   3 |  1200/ 1572 batches | rec 54.94, adv 0.68, |lvar| 439.13, loss_d 1.46, loss 61.72,\n",
      "| epoch   3 |  1300/ 1572 batches | rec 71.22, adv 0.67, |lvar| 476.25, loss_d 1.41, loss 77.89,\n",
      "| epoch   3 |  1400/ 1572 batches | rec 53.33, adv 0.73, |lvar| 471.53, loss_d 1.43, loss 60.59,\n",
      "| epoch   3 |  1500/ 1572 batches | rec 54.84, adv 0.70, |lvar| 522.10, loss_d 1.40, loss 61.84,\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time   192s | valid rec 50.92, adv 0.76, |lvar| 412.65, loss_d 1.38, loss 58.48, | saving model\n",
      "--------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/ 1572 batches | rec 53.23, adv 0.69, |lvar| 517.33, loss_d 1.41, loss 60.13,\n",
      "| epoch   4 |   200/ 1572 batches | rec 61.32, adv 0.66, |lvar| 515.57, loss_d 1.39, loss 67.96,\n",
      "| epoch   4 |   300/ 1572 batches | rec 57.86, adv 0.71, |lvar| 531.49, loss_d 1.43, loss 64.95,\n",
      "| epoch   4 |   400/ 1572 batches | rec 62.85, adv 0.68, |lvar| 522.37, loss_d 1.40, loss 69.69,\n",
      "| epoch   4 |   500/ 1572 batches | rec 54.60, adv 0.71, |lvar| 492.29, loss_d 1.40, loss 61.74,\n",
      "| epoch   4 |   600/ 1572 batches | rec 54.38, adv 0.71, |lvar| 566.89, loss_d 1.45, loss 61.45,\n",
      "| epoch   4 |   700/ 1572 batches | rec 56.67, adv 0.68, |lvar| 560.87, loss_d 1.38, loss 63.46,\n",
      "| epoch   4 |   800/ 1572 batches | rec 53.29, adv 0.72, |lvar| 581.99, loss_d 1.44, loss 60.52,\n",
      "| epoch   4 |   900/ 1572 batches | rec 58.56, adv 0.68, |lvar| 602.36, loss_d 1.42, loss 65.36,\n",
      "| epoch   4 |  1000/ 1572 batches | rec 55.82, adv 0.70, |lvar| 596.47, loss_d 1.40, loss 62.80,\n",
      "| epoch   4 |  1100/ 1572 batches | rec 48.51, adv 0.72, |lvar| 633.66, loss_d 1.45, loss 55.68,\n",
      "| epoch   4 |  1200/ 1572 batches | rec 50.44, adv 0.69, |lvar| 606.03, loss_d 1.43, loss 57.33,\n",
      "| epoch   4 |  1300/ 1572 batches | rec 50.31, adv 0.67, |lvar| 661.38, loss_d 1.44, loss 57.02,\n",
      "| epoch   4 |  1400/ 1572 batches | rec 51.40, adv 0.69, |lvar| 692.14, loss_d 1.43, loss 58.30,\n",
      "| epoch   4 |  1500/ 1572 batches | rec 48.52, adv 0.69, |lvar| 709.92, loss_d 1.44, loss 55.38,\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time   192s | valid rec 46.92, adv 0.59, |lvar| 663.18, loss_d 1.45, loss 52.80, | saving model\n",
      "--------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/ 1572 batches | rec 49.22, adv 0.68, |lvar| 670.51, loss_d 1.41, loss 56.04,\n",
      "| epoch   5 |   200/ 1572 batches | rec 48.51, adv 0.69, |lvar| 684.97, loss_d 1.42, loss 55.44,\n",
      "| epoch   5 |   300/ 1572 batches | rec 52.61, adv 0.68, |lvar| 682.43, loss_d 1.42, loss 59.40,\n",
      "| epoch   5 |   400/ 1572 batches | rec 52.52, adv 0.70, |lvar| 707.66, loss_d 1.44, loss 59.55,\n",
      "| epoch   5 |   500/ 1572 batches | rec 46.98, adv 0.70, |lvar| 718.68, loss_d 1.42, loss 53.93,\n",
      "| epoch   5 |   600/ 1572 batches | rec 54.44, adv 0.68, |lvar| 682.74, loss_d 1.36, loss 61.28,\n",
      "| epoch   5 |   700/ 1572 batches | rec 51.74, adv 0.70, |lvar| 685.47, loss_d 1.40, loss 58.73,\n",
      "| epoch   5 |   800/ 1572 batches | rec 50.72, adv 0.72, |lvar| 808.58, loss_d 1.46, loss 57.93,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   900/ 1572 batches | rec 50.76, adv 0.67, |lvar| 772.25, loss_d 1.42, loss 57.44,\n",
      "| epoch   5 |  1000/ 1572 batches | rec 47.65, adv 0.71, |lvar| 769.46, loss_d 1.43, loss 54.76,\n",
      "| epoch   5 |  1100/ 1572 batches | rec 52.04, adv 0.67, |lvar| 752.71, loss_d 1.38, loss 58.73,\n",
      "| epoch   5 |  1200/ 1572 batches | rec 46.42, adv 0.74, |lvar| 747.61, loss_d 1.46, loss 53.82,\n",
      "| epoch   5 |  1300/ 1572 batches | rec 49.52, adv 0.67, |lvar| 792.16, loss_d 1.45, loss 56.27,\n",
      "| epoch   5 |  1400/ 1572 batches | rec 45.99, adv 0.68, |lvar| 763.41, loss_d 1.39, loss 52.82,\n",
      "| epoch   5 |  1500/ 1572 batches | rec 42.87, adv 0.71, |lvar| 794.37, loss_d 1.41, loss 49.98,\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time   192s | valid rec 42.92, adv 0.73, |lvar| 810.45, loss_d 1.34, loss 50.20, | saving model\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d42bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab('checkpoints/aae/vocab.txt')\n",
    "test.set_seed(598)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path, vocab):\n",
    "    ckpt = torch.load(path)\n",
    "    train_args = ckpt['args']\n",
    "    model = test.AAE(vocab, train_args).to(device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    model.flatten()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2de80396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('checkpoints/aae/model.pt', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "784ed4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(size=(10, 128)).astype('f')\n",
    "sents = []\n",
    "i = 0\n",
    "while i < len(z):\n",
    "    zi = torch.tensor(z[i: i+1], device=device)\n",
    "    outputs = model.generate(zi, 35, 'sample').t()\n",
    "    for s in outputs:\n",
    "        sents.append([vocab.idx2word[id] for id in s[1:]])  # skip <go>\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42bd4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write_sent(sents, 'checkpoints/aae/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cec3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sents, vocab, batch_size, model, device, enc='mu'):\n",
    "    batches, order = test.get_batches(sents, vocab, batch_size, device)\n",
    "    z = []\n",
    "    for inputs, _ in batches:\n",
    "        mu, logvar = model.encode(inputs)\n",
    "        if enc == 'mu':\n",
    "            zi = mu\n",
    "        else:\n",
    "            zi = test.reparameterize(mu, logvar)\n",
    "        z.append(zi.detach().cpu().numpy())\n",
    "    z = np.concatenate(z, axis=0)\n",
    "    z_ = np.zeros_like(z)\n",
    "    z_[np.array(order)] = z\n",
    "    return z_\n",
    "\n",
    "def decode(z, vocab, batch_size, max_len, model, device, dec='sample'):\n",
    "    sents = []\n",
    "    i = 0\n",
    "    while i < len(z):\n",
    "        zi = torch.tensor(z[i: i+batch_size], device=device)\n",
    "        outputs = model.generate(zi, max_len, dec).t()\n",
    "        for s in outputs:\n",
    "            sents.append([vocab.idx2word[id] for id in s[1:]])  # skip <go>\n",
    "        i += batch_size\n",
    "    return test.strip_eos(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3829fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got all all over the last year.\n",
      "I got to put over over the years.\n",
      "I got all over over the years.\n",
      "I got all over over the years.\n",
      "I got plenty of time for it.\n",
      "I haven't gone up in it.\n",
      "I got over it over the years.\n",
      "I haven't talked to get over the time.\n",
      "I got all over the last year.\n",
      "I got to put over over time.\n"
     ]
    }
   ],
   "source": [
    "premise = \"Um, I read some of the same books that they had read to me, first, and then, as I got older, I just got hungry for books.\"\n",
    "hypothesis = \"I lost interest in reading over time.\"\n",
    "label = \"contradiction\"\n",
    "\n",
    "sents = [ hypothesis.split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, 0.2, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5c303",
   "metadata": {},
   "source": [
    "## Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36de4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab('checkpoints/aae-2023-04-11_19-20-46/vocab.txt')\n",
    "test.set_seed(598)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bac7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('checkpoints/aae-2023-04-11_19-20-46/model.pt', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b8c848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how long it got last in.\n",
      "They didn't see how long it got last day.\n",
      "I don't know how cold it went last night.\n",
      "Jon all saw the paths after just got it.\n",
      "I don't know how it had gone last night.\n",
      "The clue how it got cold it last night.\n",
      "I don't know how long it saw last night.\n",
      "I don't know how long it last night.\n",
      "I don't know how it stayed the last night.\n",
      "I knew how so it was a last night.\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"I don't know how cold it got last night.\"\n",
    "\n",
    "sents = [ hypothesis.split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, 0.6, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2f93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
