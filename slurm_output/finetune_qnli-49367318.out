03/18/2023 00:12:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/processors/glue.py:475: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format("processor"), FutureWarning)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/18/2023 00:12:26 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='glue_data/QNLI/processed', model_type='distilbert', model_name_or_path='distilbert-base-uncased', task_name='qnli', output_dir='checkpoints/', config_name='', tokenizer_name='', cache_dir='', max_seq_length=512, do_train=True, do_eval=True, evaluate_during_training=True, do_lower_case=True, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=1e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1, warmup_steps=1986, logging_steps=100, save_steps=100, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', adv_lr=0.05, adv_steps=3, adv_init_mag=0.1, norm_type='l2', adv_max_norm=0.0, gpu='0', expname='default', comet=False, comet_key='', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0, n_gpu=1, device=device(type='cuda'), output_mode='classification')
03/18/2023 00:12:26 - INFO - __main__ -   Loading features from cached file glue_data/QNLI/processed/cached_train_distilbert-base-uncased_512_qnli
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
03/18/2023 00:12:54 - INFO - __main__ -   ***** Running training *****
03/18/2023 00:12:54 - INFO - __main__ -     Num examples = 104742
03/18/2023 00:12:54 - INFO - __main__ -     Num Epochs = 1
03/18/2023 00:12:54 - INFO - __main__ -     Instantaneous batch size per GPU = 8
03/18/2023 00:12:54 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
03/18/2023 00:12:54 - INFO - __main__ -     Gradient Accumulation steps = 4
03/18/2023 00:12:54 - INFO - __main__ -     Total optimization steps = 1
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/13093 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/13093 [00:00<1:37:08,  2.25it/s][A
Iteration:   0%|          | 3/13093 [00:00<34:52,  6.25it/s]  [A
Iteration:   0%|          | 5/13093 [00:00<30:12,  7.22it/s][A
Iteration:   0%|          | 7/13093 [00:00<22:46,  9.58it/s][AIteration:   0%|          | 7/13093 [00:01<31:32,  6.91it/s]
Epoch:   0%|          | 0/1 [00:01<?, ?it/s]
03/18/2023 00:12:55 - INFO - __main__ -    global_step = 2, average loss = 0.7019633091986179
03/18/2023 00:12:55 - INFO - __main__ -   Saving model checkpoint to checkpoints/
03/18/2023 00:12:57 - INFO - __main__ -   Evaluate the following checkpoints: ['checkpoints/']
03/18/2023 00:12:58 - INFO - __main__ -   Creating features from dataset file at glue_data/QNLI/processed
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING.format("function"), FutureWarning)
03/18/2023 00:13:03 - INFO - __main__ -   Saving features into cached file glue_data/QNLI/processed/cached_dev_distilbert-base-uncased_512_qnli
03/18/2023 00:13:06 - INFO - __main__ -   ***** Running evaluation  *****
03/18/2023 00:13:06 - INFO - __main__ -     Num examples = 5462
03/18/2023 00:13:06 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/683 [00:00<?, ?it/s]Evaluating:   2%|â–         | 12/683 [00:00<00:05, 113.83it/s]Evaluating:   4%|â–         | 26/683 [00:00<00:05, 124.23it/s]Evaluating:   6%|â–Œ         | 40/683 [00:00<00:05, 128.38it/s]Evaluating:   8%|â–Š         | 53/683 [00:00<00:04, 126.86it/s]Evaluating:  10%|â–‰         | 66/683 [00:00<00:04, 123.84it/s]Evaluating:  12%|â–ˆâ–        | 80/683 [00:00<00:04, 127.84it/s]Evaluating:  14%|â–ˆâ–        | 94/683 [00:00<00:04, 128.35it/s]Evaluating:  16%|â–ˆâ–Œ        | 107/683 [00:00<00:04, 127.49it/s]Evaluating:  18%|â–ˆâ–Š        | 121/683 [00:00<00:04, 128.27it/s]Evaluating:  20%|â–ˆâ–‰        | 135/683 [00:01<00:04, 130.25it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 149/683 [00:01<00:04, 128.32it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 163/683 [00:01<00:03, 131.23it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 177/683 [00:01<00:03, 129.13it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 192/683 [00:01<00:03, 133.47it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 206/683 [00:01<00:03, 133.60it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 220/683 [00:01<00:03, 134.50it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 234/683 [00:01<00:03, 132.04it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 248/683 [00:01<00:03, 131.01it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 262/683 [00:02<00:03, 132.12it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 276/683 [00:02<00:03, 131.65it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 290/683 [00:02<00:02, 131.65it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 304/683 [00:02<00:02, 131.59it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 318/683 [00:02<00:02, 126.65it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 331/683 [00:02<00:02, 126.43it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 346/683 [00:02<00:02, 132.57it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 360/683 [00:02<00:02, 132.42it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 374/683 [00:02<00:02, 131.20it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 388/683 [00:02<00:02, 132.00it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 402/683 [00:03<00:02, 129.63it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 415/683 [00:03<00:02, 129.44it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 428/683 [00:03<00:01, 128.98it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 442/683 [00:03<00:01, 129.34it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 457/683 [00:03<00:01, 133.16it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 471/683 [00:03<00:01, 132.31it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 485/683 [00:03<00:01, 133.76it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 500/683 [00:03<00:01, 135.28it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 514/683 [00:03<00:01, 135.03it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 528/683 [00:04<00:01, 135.45it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 542/683 [00:04<00:01, 135.73it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 556/683 [00:04<00:00, 136.32it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 570/683 [00:04<00:00, 134.83it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 584/683 [00:04<00:00, 134.87it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 598/683 [00:04<00:00, 134.71it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 612/683 [00:04<00:00, 131.93it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 627/683 [00:04<00:00, 133.59it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 642/683 [00:04<00:00, 135.95it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 656/683 [00:04<00:00, 134.34it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 670/683 [00:05<00:00, 134.05it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 683/683 [00:05<00:00, 131.79it/s]
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/aakashdp/eecs598/eecs598-ood-adversarial/env/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
03/18/2023 00:13:13 - INFO - __main__ -   ***** Eval results  *****
03/18/2023 00:13:13 - INFO - __main__ -     acc = 0.49340900768949103
