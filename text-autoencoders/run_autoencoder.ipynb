{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6a2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b9bc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefa9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main\n",
    "from argparse import Namespace\n",
    "import test\n",
    "from vocab import Vocab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac2d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import glue_convert_examples_to_features\n",
    "from transformers import glue_processors\n",
    "from typing import List, Optional, Union\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e532181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: str\n",
    "    label: Optional[str] = None\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label: Optional[Union[int, float]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path, vocab):\n",
    "    ckpt = torch.load(path)\n",
    "    train_args = ckpt['args']\n",
    "    model = test.AAE(vocab, train_args).to(device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    model.flatten()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cec3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sents, vocab, batch_size, model, device, enc='mu'):\n",
    "    batches, order = test.get_batches(sents, vocab, batch_size, device)\n",
    "    z = []\n",
    "    for inputs, _ in batches:\n",
    "        mu, logvar = model.encode(inputs)\n",
    "        if enc == 'mu':\n",
    "            zi = mu\n",
    "        else:\n",
    "            zi = test.reparameterize(mu, logvar)\n",
    "        z.append(zi.detach().cpu().numpy())\n",
    "    z = np.concatenate(z, axis=0)\n",
    "    z_ = np.zeros_like(z)\n",
    "    z_[np.array(order)] = z\n",
    "    return z_\n",
    "\n",
    "def decode(z, vocab, batch_size, max_len, model, device, dec='sample'):\n",
    "    sents = []\n",
    "    i = 0\n",
    "    while i < len(z):\n",
    "        zi = torch.tensor(z[i: i+batch_size], device=device)\n",
    "        outputs = model.generate(zi, max_len, dec).t()\n",
    "        for s in outputs:\n",
    "            sents.append([vocab.idx2word[id] for id in s[1:]])  # skip <go>\n",
    "        i += batch_size\n",
    "    return test.strip_eos(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6c02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(premise, hypotheses, tokenizer):\n",
    "    processor = glue_processors['mnli']()\n",
    "    label_list = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    examples = []\n",
    "    for i, hypothesis in enumerate(hypotheses):\n",
    "        examples.append(InputExample(guid=f'test-{i}', text_a=premise, text_b=hypothesis, label='contradiction'))\n",
    "    \n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    labels = [label_map[example.label] for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "        feature = InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "    # dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d9fea",
   "metadata": {},
   "source": [
    "## Load Premise-Hypothesis pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f8e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab('../checkpoints/aae_epoch100/vocab.txt')\n",
    "test.set_seed(598)\n",
    "torch.manual_seed(598)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_model('../checkpoints/aae_epoch100/model.pt', vocab)\n",
    "\n",
    "perturb_noise = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6175ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = '../checkpoints/mnli_baseline_distilbert-2023-04-07_10-48-14/checkpoint-last'\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    classifier_path,\n",
    "    num_labels=3,\n",
    "    finetuning_task='mnli',\n",
    "    attention_probs_dropout_prob=0,\n",
    "    hidden_dropout_prob=0.1\n",
    ")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    classifier_path,\n",
    "    do_lower_case=True,\n",
    ")\n",
    "classifier = DistilBertForSequenceClassification.from_pretrained(\n",
    "    classifier_path,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56425f72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1524638/1713111959.py:6: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data/mnli/train2.tsv', sep='\\t')\n",
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: How do you know ? All this is their information again .\n",
      "Original hypothesis: This information belongs to them . --> entailment\n",
      "Best sentence: This information belongs to them Tommy. --> neutral\n",
      "\n",
      "Premise: I burst through a set of cabin doors , and fell to the ground -\n",
      "Original hypothesis: I burst through the doors and fell down . --> entailment\n",
      "Best sentence: I burst through the doors and fell down slowly. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Issues in Data Synthesis .\n",
      "Original hypothesis: Problems in data synthesis . --> entailment\n",
      "Best sentence: Problems in data is fair. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The other men shuffled .\n",
      "Original hypothesis: The other men were shuffled around . --> entailment\n",
      "Best sentence: The other men were shuffled around quicker. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: well it 's been very interesting\n",
      "Original hypothesis: It has been very intriguing . --> entailment\n",
      "Best sentence: It has very intriguing trails --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: He started slowly back to the bunkhouse .\n",
      "Original hypothesis: He returned slowly to the bunkhouse . --> entailment\n",
      "Best sentence: He returned slowly to the Red . --> neutral\n",
      "\n",
      "Premise: and it 's it 's quite a bit i think six something is the state and and uh the rest of the pie goes elsewhere but we 're in a particular part of the state that 's pretty well off so it 's it 's like we get a lot of that back as far as local taxation goes\n",
      "Original hypothesis: I do not know exactly where the local taxes go . --> neutral\n",
      "Best sentence: I do not know where the American taxes go . --> contradiction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Postal Service were to reduce delivery frequency .\n",
      "Original hypothesis: The postal service could deliver less frequently . --> entailment\n",
      "Best sentence: The postal service could deliver significantly postage . --> contradiction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Felicia 's Journey takes place behind the eyes of its central a young Irish girl , Felicia , who crosses the sea to England in a hopeful quest to find the father of her unborn child ; and the fat , middle-aged catering manager , Hiditch , who takes a paternal interest in the lass when it becomes clear that her young man has caddishly given her the slip .\n",
      "Original hypothesis: The woman did not care where the man was as long as it was far . --> contradiction\n",
      "Best sentence: The woman did not know where the man was as long as it was right --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: You have access to the facts .\n",
      "Original hypothesis: The facts are accessible to you . --> entailment\n",
      "Best sentence: The facts are easy to you . --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Build environment Engineering Manufacturing Production -LRB- all rate tooling -RRB- -LRB- 1st set of production tooling -RRB-\n",
      "Original hypothesis: It is the first set of production tooling for manufacturing . --> entailment\n",
      "Best sentence: It is the first set of production tooling for advertising. --> neutral\n",
      "\n",
      "Premise: I did not mention Monica in my lecture , but the first question I was asked was how President Clinton could do his job with all the distractions caused by the Monica Lewinsky affair .\n",
      "Original hypothesis: They wanted to get through the lecture without any problems . --> neutral\n",
      "Best sentence: They wanted to get through the faces without any issues --> contradiction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Hills and mountains are especially sanctified in the cult of Jainism .\n",
      "Original hypothesis: The cult of Jainism hates nature . --> contradiction\n",
      "Best sentence: The cult of Jainism likes nature . --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The famous tenements -LRB- or lands -RRB- began to be built .\n",
      "Original hypothesis: The land remained deserted . --> contradiction\n",
      "Best sentence: The land remained completely delicious. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Each of the men wore leather armor and dressed in the style of heavy riders .\n",
      "Original hypothesis: The men were naked . --> contradiction\n",
      "Best sentence: The men were white. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: which i mean i think it should be anyway\n",
      "Original hypothesis: I do n't think it should be that way --> contradiction\n",
      "Best sentence: I do think it should be that way --> entailment\n",
      "\n",
      "Premise: no oh no oh well take care\n",
      "Original hypothesis: Bye for now . --> entailment\n",
      "Best sentence: Sara for now . --> contradiction\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The With attorneys one year out of graduate school facing an average debt of just less than $ 90,000 and starting salaries at legal aid organizations averaging $ 31,000 , they could n't afford the job .\n",
      "Original hypothesis: New attorneys have massive law school debt . --> entailment\n",
      "Best sentence: New attorneys have a law school disability --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Here you 'll see a shrunken head , a two-headed goat , and a statue of Marilyn Monroe made of shredded money , among other curiosities .\n",
      "Original hypothesis: One of the curiosities is a two-headed goat . --> entailment\n",
      "Best sentence: One of the curiosities is a refreshing character. --> neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: right right well it 's it 's a beautiful city and but the problem is like first example when i was young they they took me to Las Vegas and that was the most boring place on earth\n",
      "Original hypothesis: I think Las Vegas is the most boring place I know . --> entailment\n",
      "Best sentence: I think Las Vegas is the most exciting place I know the . --> contradiction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import string\n",
    "import jiwer\n",
    "\n",
    "data = pd.read_csv('data/mnli/train2.tsv', sep='\\t')\n",
    "\n",
    "aug_batch = []\n",
    "aug_i = 0\n",
    "data_start = 0\n",
    "data_appended = data_start\n",
    "\n",
    "premise_seen = {}\n",
    "\n",
    "for index, row in data.loc[data_start:].iterrows():\n",
    "    # Load premise, hypothesis, and label\n",
    "    premise_text = row['sentence1']\n",
    "    raw_premise = row['sentence1_binary_parse'].split(' ')\n",
    "    try:\n",
    "        raw_hypothesis = row['sentence2_binary_parse'].split(' ')\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    orig_label = row['gold_label']\n",
    "        \n",
    "    # Process premise\n",
    "    premise_words = []\n",
    "    for word in raw_premise:\n",
    "        if word != \"(\" and word != \")\":\n",
    "            premise_words.append(word)\n",
    "    premise = \" \".join(premise_words)\n",
    "    \n",
    "    # Check that premise is unique\n",
    "    if premise in premise_seen.keys():\n",
    "        continue\n",
    "        \n",
    "    premise_seen[premise] = True\n",
    "\n",
    "    # Process hypothesis\n",
    "    hypothesis_words = []\n",
    "    for word in raw_hypothesis:\n",
    "        if word != \"(\" and word != \")\":\n",
    "            hypothesis_words.append(word)\n",
    "    hypothesis = \" \".join(hypothesis_words)\n",
    "    \n",
    "    # Generate sentences\n",
    "    sents = [ hypothesis.split() ]\n",
    "    z = encode(sents, vocab, 1, model, device)\n",
    "    n = 10\n",
    "    \n",
    "    orig_hypothesis = hypothesis\n",
    "    hypotheses = []\n",
    "    for i in range(n):\n",
    "        z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "        decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "        hypotheses.append(' '.join(decoded[0]))\n",
    "        \n",
    "    # Run classifier on new hypotheses\n",
    "    dataset = load_data(premise, hypotheses, tokenizer)\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=16)\n",
    "    for batch in eval_dataloader:\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "        _, logits = classifier(**inputs)[:2]\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    label_list = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    \n",
    "    min_dist = 1e9\n",
    "    best_sent = None\n",
    "    best_label = None\n",
    "\n",
    "    for sentence, pred in zip(hypotheses, preds):\n",
    "        if '<unk>' in sentence:\n",
    "            continue\n",
    "        \n",
    "        # Remove trailing punctuation for comparison\n",
    "        if sentence[-1] in string.punctuation:\n",
    "            sentence_comp = sentence[:-1].rstrip()\n",
    "        else:\n",
    "            sentence_comp = sentence\n",
    "        if orig_hypothesis[-1] in string.punctuation:\n",
    "            orig_hypothesis_comp = orig_hypothesis[:-1].rstrip()\n",
    "        else:\n",
    "            orig_hypothesis_comp = orig_hypothesis\n",
    "\n",
    "        # Choose best sentence based on WER to original (with different label)\n",
    "        if orig_label != label_list[pred] and orig_hypothesis_comp != sentence_comp:\n",
    "            dist = jiwer.wer(orig_hypothesis_comp, sentence_comp)\n",
    "            dist *= len(orig_hypothesis_comp.split())\n",
    "            if dist <= 2 and dist > 0:\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_sent = sentence\n",
    "                    best_label = label_list[pred]\n",
    "            \n",
    "                \n",
    "    # Skip if no close sentences were found\n",
    "    if best_sent == None:\n",
    "        continue\n",
    "\n",
    "    print('Premise: {}'.format(premise))\n",
    "    print('Original hypothesis: {} --> {}'.format(orig_hypothesis, orig_label))\n",
    "    print('Best sentence: {} --> {}\\n'.format(best_sent, best_label))\n",
    "\n",
    "    # Fill aug_data row with necessary info\n",
    "    aug_row = []\n",
    "    for i in range(8):\n",
    "        aug_row.append('')\n",
    "    aug_row.append(premise_text)\n",
    "    aug_row.append(best_sent)\n",
    "    aug_row.append('')\n",
    "    aug_row.append(orig_label)\n",
    "    aug_row.append('')\n",
    "\n",
    "    aug_batch.append(aug_row)\n",
    "    aug_i += 1\n",
    "    data_appended += 1\n",
    "    \n",
    "    # Write to TSV every 1000 lines in case it's too slow overall\n",
    "    if aug_i >= 500:\n",
    "        aug_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "        # Add data from batch\n",
    "        for batch_i, batch_row in enumerate(aug_batch):\n",
    "            aug_data.loc[batch_i] = batch_row\n",
    "        \n",
    "        aug_data.to_csv('data/aug{}.tsv'.format(data_appended), sep=\"\\t\")\n",
    "        \n",
    "        # Reset aug data and counter\n",
    "        aug_data = pd.DataFrame(columns=data.columns)\n",
    "        aug_batch = []\n",
    "        aug_i = 0\n",
    "        \n",
    "    if data_appended >= data.shape[0] * 0.05:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5c303",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea2f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how much it last night got like.\n",
      "I don't know how much it last night was said.\n",
      "I don't know how much it cold last night last night\n",
      "I don't know how much it last night was\n",
      "I don't know how much it last night last night\n",
      "I don't know how much it was last night\n",
      "I don't know how it got cold last night\n",
      "I don't know how much it came last night again.\n",
      "I don't know how long it was night again.\n",
      "I don't know how much it last night was loud.\n"
     ]
    }
   ],
   "source": [
    "hypotheses = []\n",
    "hypotheses.append(\"I don't know how cold it got last night .\")\n",
    "\n",
    "sents = [ hypotheses[0].split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "\n",
    "    hypotheses.append(' '.join(decoded[0]))\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8aa1054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product and geography are what makes it go ahead and programming\n",
      "Product and geography are the quickest to work poorly.\n",
      "Product and geography are the quickest location of weight and Windows .\n",
      "Product and geography are what makes it go .\n",
      "Home and geography are what to make weight .\n",
      "Fiscal and geography are what mailers can be programming programming is.\n",
      "Product and geography are the reason to weight programming .\n",
      "Product and geography are what make it go home programming\n",
      "Product and geography are what mailers go Windows programming\n",
      "Product and geography are the quickest to make weight programming .\n"
     ]
    }
   ],
   "source": [
    "hypotheses = []\n",
    "hypotheses.append(\"Product and geography are what make cream skimming work .\")\n",
    "\n",
    "sents = [ hypotheses[0].split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80a62a",
   "metadata": {},
   "source": [
    "## Find labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a45ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"Postal Service were to reduce delivery frequency.\"\n",
    "orig_hypothesis = \"The postal service could deliver less frequently.\"\n",
    "orig_label = \"entailment\"\n",
    "hypotheses = [\n",
    "    \"The postal service could deliver significantly often.\"\n",
    "]\n",
    "\n",
    "dataset = load_data(premise, hypotheses, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7d2ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(dataset, batch_size=16)\n",
    "for batch in eval_dataloader:\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "    _, logits = classifier(**inputs)[:2]\n",
    "    preds = logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(preds.tolist())\n",
    "    \n",
    "label_list = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9a64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: Postal Service were to reduce delivery frequency.\n",
      "Original hypothesis: The postal service could deliver less frequently.\n",
      "Label: entailment\n",
      "--------------------\n",
      "The postal service could deliver significantly often. --> contradiction\n"
     ]
    }
   ],
   "source": [
    "print(f'Premise: {premise}')\n",
    "print(f'Original hypothesis: {orig_hypothesis}')\n",
    "print(f'Label: {orig_label}')\n",
    "print('--------------------')\n",
    "for sentence, pred in zip(hypotheses, preds):\n",
    "    print(f'{sentence} --> {label_list[pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0bc43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Premise: {premise}')\n",
    "print(f'Original hypothesis: {orig_hypothesis}')\n",
    "print(f'Label: {orig_label}')\n",
    "print('--------------------')\n",
    "for sentence, pred in zip(hypotheses, preds):\n",
    "    print(f'{sentence} --> {label_list[pred]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f29f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how cold it got last night\n",
      "I don't know how long it got last night\n",
      "I don't know how cold it got last night\n",
      "I don't know how how it last night was yesterday\n",
      "I don't know how cold it got last night\n",
      "I don't know how bad it last night constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it when last night cup\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it last night got wet\n",
      "I don't know how cold it got last night\n",
      "I don't know how much water it last night night\n",
      "I don't know how cold it got last night\n",
      "I don't know how bad it last night got constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it last night went constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how long it got last night\n",
      "Original hypothesis: I don't know how cold it got last night.\n",
      "best sentence: I don't know how long it got last night . --> neutral\n"
     ]
    }
   ],
   "source": [
    "min_dist = 1e9\n",
    "best_sent = ''\n",
    "best_label = ''\n",
    "\n",
    "for sentence, pred in zip(hypotheses, preds):\n",
    "    # Remove trailing punctuation for comparison\n",
    "    if sentence[-1] in string.punctuation:\n",
    "        sentence_comp = sentence[:-1].rstrip()\n",
    "    else:\n",
    "        sentence_comp = sentence\n",
    "    if orig_hypothesis[-1] in string.punctuation:\n",
    "        orig_hypothesis_comp = orig_hypothesis[:-1].rstrip()\n",
    "    else:\n",
    "        orig_hypothesis_comp = orig_hypothesis\n",
    "            \n",
    "    if orig_label != label_list[pred] and orig_hypothesis_comp != sentence_comp:\n",
    "        dist = Levenshtein.distance(orig_hypothesis_comp, sentence_comp)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_sent = sentence\n",
    "            best_label = label_list[pred]\n",
    "                    \n",
    "print(f'Original hypothesis: {orig_hypothesis}')\n",
    "print('best sentence: {} --> {}'.format(best_sent, best_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6552173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
