{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6a2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b9bc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefa9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main\n",
    "from argparse import Namespace\n",
    "import test\n",
    "from vocab import Vocab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac2d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import glue_convert_examples_to_features\n",
    "from transformers import glue_processors\n",
    "from typing import List, Optional, Union\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e532181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: str\n",
    "    label: Optional[str] = None\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class InputFeatures:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    label: Optional[Union[int, float]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path, vocab):\n",
    "    ckpt = torch.load(path)\n",
    "    train_args = ckpt['args']\n",
    "    model = test.AAE(vocab, train_args).to(device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    model.flatten()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cec3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sents, vocab, batch_size, model, device, enc='mu'):\n",
    "    batches, order = test.get_batches(sents, vocab, batch_size, device)\n",
    "    z = []\n",
    "    for inputs, _ in batches:\n",
    "        mu, logvar = model.encode(inputs)\n",
    "        if enc == 'mu':\n",
    "            zi = mu\n",
    "        else:\n",
    "            zi = test.reparameterize(mu, logvar)\n",
    "        z.append(zi.detach().cpu().numpy())\n",
    "    z = np.concatenate(z, axis=0)\n",
    "    z_ = np.zeros_like(z)\n",
    "    z_[np.array(order)] = z\n",
    "    return z_\n",
    "\n",
    "def decode(z, vocab, batch_size, max_len, model, device, dec='sample'):\n",
    "    sents = []\n",
    "    i = 0\n",
    "    while i < len(z):\n",
    "        zi = torch.tensor(z[i: i+batch_size], device=device)\n",
    "        outputs = model.generate(zi, max_len, dec).t()\n",
    "        for s in outputs:\n",
    "            sents.append([vocab.idx2word[id] for id in s[1:]])  # skip <go>\n",
    "        i += batch_size\n",
    "    return test.strip_eos(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6c02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(premise, hypotheses, tokenizer):\n",
    "    processor = glue_processors['mnli']()\n",
    "    label_list = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    examples = []\n",
    "    for i, hypothesis in enumerate(hypotheses):\n",
    "        examples.append(InputExample(guid=f'test-{i}', text_a=premise, text_b=hypothesis, label='contradiction'))\n",
    "    \n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    labels = [label_map[example.label] for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "        feature = InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "    # dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d9fea",
   "metadata": {},
   "source": [
    "## Load Premise-Hypothesis pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f8e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab('../checkpoints/aae_epoch100/vocab.txt')\n",
    "test.set_seed(598)\n",
    "torch.manual_seed(598)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = get_model('../checkpoints/aae_epoch100/model.pt', vocab)\n",
    "\n",
    "perturb_noise = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6175ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = '../checkpoints/mnli_baseline_distilbert-2023-04-07_10-48-14/checkpoint-last'\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    classifier_path,\n",
    "    num_labels=3,\n",
    "    finetuning_task='mnli',\n",
    "    attention_probs_dropout_prob=0,\n",
    "    hidden_dropout_prob=0.1\n",
    ")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    classifier_path,\n",
    "    do_lower_case=True,\n",
    ")\n",
    "classifier = DistilBertForSequenceClassification.from_pretrained(\n",
    "    classifier_path,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56425f72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2493970/1890078508.py:6: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data/mnli/train2.tsv', sep='\\t')\n",
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: How do you know ? All this is their information again .\n",
      "Original hypothesis: This information belongs to them . --> entailment\n",
      "Best sentence: This information belongs to them Tommy. --> neutral\n",
      "\n",
      "Premise: yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they 're getting up in the hundred dollar range\n",
      "Original hypothesis: The tennis shoes have a range of prices . --> neutral\n",
      "Best sentence: The tennis manufacturers have a range of prices . --> entailment\n",
      "\n",
      "Premise: I burst through a set of cabin doors , and fell to the ground -\n",
      "Original hypothesis: I burst through the doors and fell down . --> entailment\n",
      "Best sentence: I burst through the doors and fell forward. --> contradiction\n",
      "\n",
      "Premise: Issues in Data Synthesis .\n",
      "Original hypothesis: Problems in data synthesis . --> entailment\n",
      "Best sentence: Problems in data wrong. --> neutral\n",
      "\n",
      "Premise: The other men shuffled .\n",
      "Original hypothesis: The other men were shuffled around . --> entailment\n",
      "Best sentence: The other men were shuffled around quicker. --> neutral\n",
      "\n",
      "Premise: well it 's been very interesting\n",
      "Original hypothesis: It has been very intriguing . --> entailment\n",
      "Best sentence: It has been very intriguing to Tommy. --> neutral\n",
      "\n",
      "Premise: He started slowly back to the bunkhouse .\n",
      "Original hypothesis: He returned slowly to the bunkhouse . --> entailment\n",
      "Best sentence: She returned slowly to the bunkhouse . --> neutral\n",
      "\n",
      "Premise: and it 's it 's quite a bit i think six something is the state and and uh the rest of the pie goes elsewhere but we 're in a particular part of the state that 's pretty well off so it 's it 's like we get a lot of that back as far as local taxation goes\n",
      "Original hypothesis: I do not know exactly where the local taxes go . --> neutral\n",
      "Best sentence: I do not know where the American taxes go . --> contradiction\n",
      "\n",
      "Premise: Postal Service were to reduce delivery frequency .\n",
      "Original hypothesis: The postal service could deliver less frequently . --> entailment\n",
      "Best sentence: The postal service could deliver significantly less than . --> contradiction\n",
      "\n",
      "Premise: Felicia 's Journey takes place behind the eyes of its central a young Irish girl , Felicia , who crosses the sea to England in a hopeful quest to find the father of her unborn child ; and the fat , middle-aged catering manager , Hiditch , who takes a paternal interest in the lass when it becomes clear that her young man has caddishly given her the slip .\n",
      "Original hypothesis: The woman did not care where the man was as long as it was far . --> contradiction\n",
      "Best sentence: The boy did not care where the man was as long as it was moving . --> neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import string\n",
    "import jiwer\n",
    "\n",
    "data = pd.read_csv('data/mnli/train2.tsv', sep='\\t')\n",
    "\n",
    "premise_seen = {}\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if index > 39:\n",
    "        break\n",
    "    \n",
    "    # Load premise, hypothesis, and label\n",
    "    raw_premise = row['sentence1_binary_parse'].split(' ')\n",
    "    try:\n",
    "        raw_hypothesis = row['sentence2_binary_parse'].split(' ')\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    orig_label = row['gold_label']\n",
    "        \n",
    "    # Process premise\n",
    "    premise_words = []\n",
    "    for word in raw_premise:\n",
    "        if word != \"(\" and word != \")\":\n",
    "            premise_words.append(word)\n",
    "    premise = \" \".join(premise_words)\n",
    "    \n",
    "    # Check that premise is unique\n",
    "    if premise in premise_seen.keys():\n",
    "        continue\n",
    "        \n",
    "    premise_seen[premise] = True\n",
    "\n",
    "    # Process hypothesis\n",
    "    hypothesis_words = []\n",
    "    for word in raw_hypothesis:\n",
    "        if word != \"(\" and word != \")\":\n",
    "            hypothesis_words.append(word)\n",
    "    hypothesis = \" \".join(hypothesis_words)\n",
    "    \n",
    "    # Generate sentences\n",
    "    sents = [ hypothesis.split() ]\n",
    "    z = encode(sents, vocab, 1, model, device)\n",
    "    n = 10\n",
    "    \n",
    "    orig_hypothesis = hypothesis\n",
    "    hypotheses = []\n",
    "    for i in range(n):\n",
    "        z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "        decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "        hypotheses.append(' '.join(decoded[0]))\n",
    "        \n",
    "    # Run classifier on new hypotheses\n",
    "    dataset = load_data(premise, hypotheses, tokenizer)\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=16)\n",
    "    for batch in eval_dataloader:\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "        _, logits = classifier(**inputs)[:2]\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    label_list = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    \n",
    "    min_dist = 1e9\n",
    "    best_sent = None\n",
    "    best_label = None\n",
    "\n",
    "    for sentence, pred in zip(hypotheses, preds):\n",
    "        if '<unk>' in sentence:\n",
    "            continue\n",
    "        \n",
    "        # Remove trailing punctuation for comparison\n",
    "        if sentence[-1] in string.punctuation:\n",
    "            sentence_comp = sentence[:-1].rstrip()\n",
    "        else:\n",
    "            sentence_comp = sentence\n",
    "        if orig_hypothesis[-1] in string.punctuation:\n",
    "            orig_hypothesis_comp = orig_hypothesis[:-1].rstrip()\n",
    "        else:\n",
    "            orig_hypothesis_comp = orig_hypothesis\n",
    "\n",
    "        # Adversarial samples change based on what the original label was  \n",
    "#         if orig_label == 'neutral':\n",
    "        label_comparison = orig_label != label_list[pred]\n",
    "#         elif orig_label == 'entailment':\n",
    "#             label_comparison = label_list[pred] == 'contradiction'\n",
    "#         elif orig_label == 'contradiction':\n",
    "#             label_comparison = label_list[pred] == 'entailment'\n",
    "#         else:\n",
    "#             label_comparison = label_list[pred] != 'neutral'\n",
    "        \n",
    "        # Choose best sentence based on edit distance to original\n",
    "        if label_comparison and orig_hypothesis_comp != sentence_comp:\n",
    "            dist = jiwer.wer(orig_hypothesis_comp, sentence_comp)\n",
    "#             dist = Levenshtein.distance(orig_hypothesis_comp, sentence_comp)\n",
    "            dist *= len(orig_hypothesis_comp.split())\n",
    "            if dist <= 2 and dist > 0:\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_sent = sentence\n",
    "                    best_label = label_list[pred]\n",
    "            \n",
    "                \n",
    "    # Skip if no close sentences were found\n",
    "    if best_sent == None:\n",
    "        continue\n",
    "\n",
    "    print('Premise: {}'.format(premise))\n",
    "    print('Original hypothesis: {} --> {}'.format(orig_hypothesis, orig_label))\n",
    "    print('Best sentence: {} --> {}\\n'.format(best_sent, best_label))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5c303",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea2f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how much it last night got like.\n",
      "I don't know how much it last night was said.\n",
      "I don't know how much it cold last night last night\n",
      "I don't know how much it last night was\n",
      "I don't know how much it last night last night\n",
      "I don't know how much it was last night\n",
      "I don't know how it got cold last night\n",
      "I don't know how much it came last night again.\n",
      "I don't know how long it was night again.\n",
      "I don't know how much it last night was loud.\n"
     ]
    }
   ],
   "source": [
    "hypotheses = []\n",
    "hypotheses.append(\"I don't know how cold it got last night .\")\n",
    "\n",
    "sents = [ hypotheses[0].split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "\n",
    "    hypotheses.append(' '.join(decoded[0]))\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8aa1054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product and geography are what makes it go ahead and programming\n",
      "Product and geography are the quickest to work poorly.\n",
      "Product and geography are the quickest location of weight and Windows .\n",
      "Product and geography are what makes it go .\n",
      "Home and geography are what to make weight .\n",
      "Fiscal and geography are what mailers can be programming programming is.\n",
      "Product and geography are the reason to weight programming .\n",
      "Product and geography are what make it go home programming\n",
      "Product and geography are what mailers go Windows programming\n",
      "Product and geography are the quickest to make weight programming .\n"
     ]
    }
   ],
   "source": [
    "hypotheses = []\n",
    "hypotheses.append(\"Product and geography are what make cream skimming work .\")\n",
    "\n",
    "sents = [ hypotheses[0].split() ]\n",
    "z = encode(sents, vocab, 1, model, device)\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    z_noise = z + np.random.normal(0, perturb_noise, size=z.shape).astype('f')\n",
    "    decoded = decode(z_noise, vocab, 1, 30, model, device, dec='greedy')\n",
    "    print(' '.join(decoded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80a62a",
   "metadata": {},
   "source": [
    "## Find labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a45ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awei/.local/lib/python3.9/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "premise = \"i'm not sure what the overnight low was\"\n",
    "orig_hypothesis = \"I don't know how cold it got last night.\"\n",
    "orig_label = \"entailment\"\n",
    "# hypotheses = [\n",
    "#     \"They didn't see how long it got last day.\",\n",
    "#     \"I don't know how cold it went last night.\",\n",
    "#     \"I don't know how it had gone last night.\",\n",
    "#     \"I don't know how it stayed the last night.\",\n",
    "#     \"I knew how so it was a last night.\"\n",
    "# ]\n",
    "\n",
    "dataset = load_data(premise, hypotheses, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee7d2ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "eval_dataloader = DataLoader(dataset, batch_size=16)\n",
    "for batch in eval_dataloader:\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "    _, logits = classifier(**inputs)[:2]\n",
    "    preds = logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(preds.tolist())\n",
    "    \n",
    "label_list = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac9a64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: i'm not sure what the overnight low was\n",
      "Original hypothesis: I don't know how cold it got last night.\n",
      "Label: entailment\n",
      "--------------------\n",
      "I don't know how cold it got last night . --> neutral\n",
      "I don't know how long it got last night . --> neutral\n",
      "I don't know how how it last night was yesterday. --> neutral\n",
      "I don't know how bad it last night constantly. --> neutral\n",
      "I don't know how much it when last night cup --> neutral\n",
      "I don't know how much it last night got wet. --> neutral\n",
      "I don't know how cold it got last night --> neutral\n",
      "I don't know how much water it last night night. --> neutral\n",
      "I don't know how bad it last night got constantly. --> neutral\n",
      "I don't know how much it last night went constantly. --> neutral\n",
      "I don't know how long it got last night --> neutral\n"
     ]
    }
   ],
   "source": [
    "print(f'Premise: {premise}')\n",
    "print(f'Original hypothesis: {orig_hypothesis}')\n",
    "print(f'Label: {orig_label}')\n",
    "print('--------------------')\n",
    "for sentence, pred in zip(hypotheses, preds):\n",
    "    print(f'{sentence} --> {label_list[pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0bc43aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f29f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how cold it got last night\n",
      "I don't know how long it got last night\n",
      "I don't know how cold it got last night\n",
      "I don't know how how it last night was yesterday\n",
      "I don't know how cold it got last night\n",
      "I don't know how bad it last night constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it when last night cup\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it last night got wet\n",
      "I don't know how cold it got last night\n",
      "I don't know how much water it last night night\n",
      "I don't know how cold it got last night\n",
      "I don't know how bad it last night got constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how much it last night went constantly\n",
      "I don't know how cold it got last night\n",
      "I don't know how long it got last night\n",
      "Original hypothesis: I don't know how cold it got last night.\n",
      "best sentence: I don't know how long it got last night . --> neutral\n"
     ]
    }
   ],
   "source": [
    "min_dist = 1e9\n",
    "best_sent = ''\n",
    "best_label = ''\n",
    "\n",
    "for sentence, pred in zip(hypotheses, preds):\n",
    "    # Remove trailing punctuation for comparison\n",
    "    if sentence[-1] in string.punctuation:\n",
    "        sentence_comp = sentence[:-1].rstrip()\n",
    "    else:\n",
    "        sentence_comp = sentence\n",
    "    if orig_hypothesis[-1] in string.punctuation:\n",
    "        orig_hypothesis_comp = orig_hypothesis[:-1].rstrip()\n",
    "    else:\n",
    "        orig_hypothesis_comp = orig_hypothesis\n",
    "            \n",
    "    if orig_label != label_list[pred] and orig_hypothesis_comp != sentence_comp:\n",
    "        dist = Levenshtein.distance(orig_hypothesis_comp, sentence_comp)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_sent = sentence\n",
    "            best_label = label_list[pred]\n",
    "                    \n",
    "print(f'Original hypothesis: {orig_hypothesis}')\n",
    "print('best sentence: {} --> {}'.format(best_sent, best_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6552173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
